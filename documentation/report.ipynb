{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Group Project\n",
    "\n",
    "Grzegorz Malisz & Tiemon Steeghs\n",
    "\n",
    "[GitHub Repository](https://github.com/grzgm/deth-group-project)\n",
    "\n",
    "## Context\n",
    "\n",
    "We are focusing on creating a model that would ease and enhance the process of learning from MOOCs [Massive Open Online Courses]. The main goal of the model is to find the best learning path for user, based on user's current profile and his goal of finishing certain course. The path established by the agent should minimize the dropout possibility, learning curve and maximise the speed of learning. For our project we have picked the domain of IT MOOCs, but our solution can be extended to other topics.\n",
    "\n",
    "## IT MOOC\n",
    "\n",
    "For the development purpose we have used this structure of MOOC:\n",
    "\n",
    "<!--!!!TODO\n",
    "1. Statistical Methods in Number Theory for Beginners\n",
    "   - Arithmetic\n",
    "   - Statistics\n",
    "2. Algebraic Statistics\n",
    "   - Algebra\n",
    "   - Statistics\n",
    "3. Basic Arithmetic\n",
    "   - Arithmetic\n",
    "   - Geometry\n",
    "   - Algebra\n",
    "   - Calculus\n",
    "4. Elementary Algebra\n",
    "   - Arithmetic\n",
    "   - Geometry\n",
    "   - Algebra\n",
    "5. Geometry:\n",
    "   - Arithmetic\n",
    "   - Geometry\n",
    "!!!TODO-->\n",
    "\n",
    "## MDP Formal Definition\n",
    "\n",
    "### States\n",
    "\n",
    "Skill levels in various domains: $(s_1, s_2, \\ldots, s_N)$\n",
    "\n",
    "Defined IT skills:\n",
    "\n",
    "<!--!!!TODO\n",
    "1. Arithmetic\n",
    "2. Geometry\n",
    "3. Algebra\n",
    "4. Calculus\n",
    "\n",
    "Descriptive visualisation of the state: $(Arithmetic, Geometry, Algebra, Calculus, Statistics, Discrete Mathematics, Logic, Mathematical Analysis)$\n",
    "\n",
    "Later in the paper we will use the two notation, which ease understanding and writing while serving different purposes. For example, to refer to the Arithmetic skill level we will use $s_1$, as well as $s_{Arithmetic}$ notation.\n",
    "!!!TODO-->\n",
    "\n",
    "### Actions\n",
    "Actions are defined by taking specific courses, each course has an associated upskilling vector $u_i$ and requirement vector $r_i$. Note that if value of the certain skill in the vector is 0, it is not denoted for the sake of clarity.\n",
    "\n",
    "<!--!!!TODO\n",
    "1. Statistical Methods in Number Theory for Beginners\n",
    "   - Requirements vector:\n",
    "     - $r_{Arithmetic} = 1$\n",
    "     - $r_{Statistics} = 1$\n",
    "   - Upskill vector:\n",
    "     - $u_{Arithmetic} = 2$\n",
    "     - $u_{Statistics} = 2$\n",
    "2. Algebraic Statistics\n",
    "   - Requirements vector:\n",
    "     - $r_{Algebra} = 2$\n",
    "     - $r_{Statistics} = 3$\n",
    "   - Upskill vector:\n",
    "     - $u_{Algebra} = 4$\n",
    "     - $u_{Statistics} = 4$\n",
    "3. Basic Arithmetic\n",
    "   - Requirements vector:\n",
    "     - None\n",
    "   - Upskill vector:\n",
    "     - $u_{Arithmetic} = 3$\n",
    "     - $u_{Geometry} = 1$\n",
    "     - $u_{Algebra} = 1$\n",
    "     - $u_{Calculus} = 1$\n",
    "4. Elementary Algebra\n",
    "   - Requirements vector:\n",
    "     - None\n",
    "   - Upskill vector:\n",
    "     - $u_{Arithmetic} = 1$\n",
    "     - $u_{Geometry} = 1$\n",
    "     - $u_{Algebra} = 2$\n",
    "5. Geometry:\n",
    "   - Requirements vector:\n",
    "     - $r_{Arithmetic} = 1$\n",
    "   - Upskill vector:\n",
    "     - $u_{Arithmetic} = 2$\n",
    "     - $u_{Geometry} = 2$\n",
    "!!!TODO-->\n",
    "\n",
    "### Transition Probabilities\n",
    "\n",
    "Probability of passing a course is proportional to the dot product of the student's skill level and the course's requirement vector. \n",
    "- If the student passes the course, their skills are updated by $s + \\alpha \\cdot u_i \\cdot x$ where $x$ is a random soft mask and $\\alpha > \\beta$.\n",
    "- If the student fails, they might still get a slight skill improvement $s + \\beta \\cdot u_i \\cdot x$ where $\\beta < \\alpha$\n",
    "\n",
    "### Terminal State Probability\n",
    "\n",
    "The transition probability for passing a course can be represented as:  \n",
    "$$P(s, s^ \\prime, a_i) = \\frac{1}{1+ \\exp(-\\gamma r_i \\cdot s)} $$\n",
    "\n",
    "where $a_i$ is the action of taking course with requirement $r_i$ and course learning outcomes $r_i$  \n",
    "\n",
    "The state transition for passing the course:  \n",
    "$$ s^ \\prime = s + \\alpha \\cdot u_i \\odot x $$\n",
    "\n",
    "And for failing:  \n",
    "$$ s^ \\prime = s + \\beta \\cdot u_i \\odot x $$\n",
    "\n",
    "where $x$ is a mask that attenuates entries of $m$ by element wise multiplication $\\odot$ . Each student has its own $x$ such that each student has its own learning abilities.\n",
    "\n",
    "The terminal state probability involves the student reaching the desired minimal skill level in all domains, leading to a reward of +1.\n",
    "\n",
    "### Rewards\n",
    "\n",
    "<!--!!!TODO\n",
    "The rewards for the agent will be based on a few factors:\n",
    "- Passing or failing a course\n",
    "- How long the agent is learning (cost of living)\n",
    "- Achieving the requested goal\n",
    "\n",
    "Passing every course costs -1, and reaching the end state grants a reward of +1. To incentivize the agent to find the most efficient route a cost of living is taking into account. We made the decision to not implement this through states (as a budget for example) but through reward logic. Implementing this feature through states will lead to a lot more different states to calculate which is something we do not want, especially when the functionality is the same.\n",
    "!!!TODO-->\n",
    "\n",
    "## Implementation\n",
    "\n",
    "### Mdp Class\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "efa745291a58343c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "b085af6d9619a111"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Python `Mdp` Class is used mainly for storage of the States, Actions, Transition Probabilities and Rewards. In addition, it encapsulates logic of determining next State after Agents Action, for the Q-Learning and Monte-Carlo algorithms, helper methods for determining the available Actions for Agent and guard method for inspecting Transition Probabilities, so that they add up to 1 or 0 in case of the terminal state.\n",
    "\n",
    "The `Mdp` Class is also taking advantage of the `EnvironmentBuilder` Class to build States, Actions, Transition Probabilities and Rewards from the given input of MOOC structure and formulas written above.\n",
    "\n",
    "The MDP is devised to work with any type of the States and Actions. It is done by assigning to each given State or Action unique Id, which is used by the MDP for all operations. This allows to store the States and Actions in the NumPy Array, to save the computation resources and allow for great extensibility. In the implementation it is done via usage of the Python Lists, which store all the Possible States and Actions and therefore assign to each unique index.\n",
    "\n",
    "### EnvironmentBuilder Class"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dbf9a353baec809f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "7ee6b26f4a0ca5b0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<!--!!!TODO\n",
    "Description of EnvironmentBuilder\n",
    "!!!TODO-->\n",
    "\n",
    "### Solver Class"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "21650a2df145bac2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "723347496a25e3e9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "`Solver Class` is used to structure and make accessible algorithms for finding the Optimal Policy. It works based on the data stored in the `Mdp` Class and consists of implementation of Dynamic Programming, Q-Learning and Monte-Carlo. Q-Learning and Monte-Carlo are implemented with two operation modes: based on fixed amount of episodes or until Policy has Converged.\n",
    "\n",
    "The `Solver Class` is also responsible for generating the plots of the Action Value Function and Episode Returns. Each plot has visible Algorithm and Parameters used to obtain data, which allows for easy reruns of Algorithms and straightforward testing Algorithm performance.\n",
    "\n",
    "`Solver Class` is mainly used with combination of three steps:\n",
    "1. Resetting Solver to the initial values,\n",
    "2. Running selected Algorithm with given values,\n",
    "3. Displaying Plots.\n",
    "\n",
    "## Algorithms\n",
    "\n",
    "For the purpose of solving the problem we have implemented 3 different algorithms:\n",
    "\n",
    "- Dynamic Programming,\n",
    "- Q-Learning,\n",
    "- Monte-Carlo.\n",
    "\n",
    "Algorithms can be categorized based on their knowledge of transition probability. Dynamic Programming uses knowledge of all transition probabilities, where Q-Learning and Monte-Carlo work based on the data provided from mdp (states, actions, rewards).\n",
    "\n",
    "In addition, to standard implementation of Q-Learning and Monte-Carlo with the Epsilon exploration, the guards for starting stage of algorithms were introduced, as if all Actions in the current state have Action Value of 0, the Agent chooses random Action instead of always first one.\n",
    "`not np.max(self.action_value_array[previous_state, :])`\n",
    "\n",
    "### Monte-Carlo\n",
    "\n",
    "It is controlled by the variable `monte_carlo_enabled`. During the episode of the Agent, a history of choices is recorded, and after the episode finishes the Action Value Function is evaluated, by analysing the Agent path from the end to the beginning. Algorithm inspects whether maximal difference between old and new value is smaller than threshold `theta`, which indicates that the optimal Action Value Function for given policy has been found. Process stops repeating when the Policy has Converged. Monte-Carlo Evaluation formula:\n",
    "\n",
    "$$\n",
    "Q({s}^m_t, {a}^m_t) \\gets Q({s}^m_t, {a}^m_t) + \\alpha ({g}^m_t - Q({s}^m_t, {a}^m_t))\n",
    "$$\n",
    "\n",
    "### Q-Learning\n",
    "\n",
    "It is controlled by the variable `q_learning_enabled` and evaluates the Action Value Function every step of the agent. Algorithm inspects whether maximal difference between old and new value is smaller than threshold `theta`, which indicates that the optimal Action Value Function for given policy has been found. Process stops repeating when the Policy has Converged. Q-Learning Evaluation formula:\n",
    "\n",
    "<!--!!!TODO $$\n",
    "action_value_array[previous_state, action] = (1 - alpha_q_learning) * action_value_array[\n",
    "                previous_state, action] + alpha_q_learning * (reward + gamma * max(action_value_array[new_state, :]))\n",
    "$$ !!!TODO-->\n",
    "\n",
    "### Dynamic Programming\n",
    "\n",
    "It is controlled by the variable `dynamic_programming_enabled` and does the Policy Evaluation and Policy Improvement to compute the Action Value Function based on the Greedy Policy with respect to Action Value Function, by implementation of the Bellman Equation:\n",
    "\n",
    "$$\n",
    "q_{\\pi}(s,a)=\\sum_{^{s^ \\prime \\in S}_{r \\in R}} p(s^ \\prime, r | s, a)[r + \\gamma \\sum_{a^ \\prime \\in A(s^ \\prime)} \\pi(a^ \\prime | s^ \\prime)q_{\\pi}(s^ \\prime | a^ \\prime)]\n",
    "$$\n",
    "\n",
    "The $\\sum_{a^ \\prime \\in A(s^ \\prime)} \\pi(a^ \\prime | s^ \\prime)q_{\\pi}(s^ \\prime | a^ \\prime)$ inside the code is solved by usage of `action_value_array[new_state, np.argmax(action_value_array[new_state, :])]`, as the Greedy Policy chooses only one action, not any other, which means that in probabilities given by $\\pi(a^ \\prime | s^ \\prime)$ they compose only of 0 and one 1, thus resulting in multiplying most of the values from the Action Value Function by 0. In order to omit unnecessary computation only the value of Action Value Function for the Action with probability of 1 is present.\n",
    "\n",
    "#### Policy Evaluation\n",
    "\n",
    "After each sweep through the Action Value Function the algorithm inspects whether maximal difference between old and new value is smaller than threshold `theta`, which indicates that the optimal Action Value Function for given policy has been found. Process stops repeating when the Policy has Converged.\n",
    "\n",
    "#### Policy Improvement / Policy Extraction\n",
    "\n",
    "Policy Improvement is done somewhat automatically, as the Policy is not stored, but is always calculated with the usage of `argmax()` function, which always returns Optimal Greedy Policy with respect to current Action Value Function.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "The model is incorporating the test data, and the agent is learning the optimal path to achieve the goal, but the model need further testing on more diverse scenarios. The ground basics for the project has been laid out. The concept of this Report can be extended to compensate possible users with not just the optimal learning path for their goal, but also it could balance between user goal and the most optimal skills for the current market. This approach would mean sacrificing user expectations, for his own good and possible success.\n",
    "\n",
    "## Addtional Notes\n",
    "\n",
    "# Redoing the same courses"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3de21ef495c936bc"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

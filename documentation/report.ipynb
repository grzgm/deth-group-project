{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Group Project\n",
    "\n",
    "Grzegorz Malisz & Tiemon Steeghs\n",
    "\n",
    "[GitHub Repository](https://github.com/grzgm/deth-group-project)\n",
    "\n",
    "## Context\n",
    "\n",
    "We are focusing on creating a model that would ease and enhance the process of learning from MOOCs [Massive Open Online Courses]. The main goal of the model is to find the best learning path for user, based on user's current profile and his goal of finishing certain course. The path established by the agent should minimize the dropout possibility, learning curve and maximise the speed of learning. For our project we have picked the domain of IT MOOCs, but our solution can be extended to other topics as can be seen in the alternative courses section.\n",
    "\n",
    "## IT MOOC\n",
    "\n",
    "For the development purpose we have used this structure of MOOC:\n",
    "   \n",
    "1. Python Programming Basics\n",
    "   - Python programming fundamentals\n",
    "\n",
    "2. Database Management Essentials\n",
    "   - Database management\n",
    "\n",
    "3. Introduction to Machine Learning with Python\n",
    "   - Python programming fundamentals\n",
    "   - Machine learning\n",
    "        \n",
    "4. Web Development Using Python\n",
    "   - Python programming fundamentals\n",
    "   - Web development\n",
    "\n",
    "\n",
    "## MDP Formal Definition\n",
    "\n",
    "### States\n",
    "\n",
    "Skill levels in various domains: $(s_1, s_2, \\ldots, s_N)$\n",
    "\n",
    "Defined IT skills:\n",
    "\n",
    "\n",
    "1. Python Programming Fundamentals\n",
    "2. Database Management\n",
    "3. Machine Learning\n",
    "4. Web Development\n",
    "\n",
    "Descriptive visualisation of the state: $(Python Programming Fundamentals, Database Management, Machine Learning, Web Development)$\n",
    "\n",
    "Later in the paper we will use the two notation, which ease understanding and writing while serving different purposes. For example, to refer to the Database Management skill level we will use $s_1$, as well as $s_{Database Management}$ notation.\n",
    "\n",
    "\n",
    "### Actions\n",
    "Actions are defined by taking specific courses, each course has an associated upskilling vector $u_i$ and requirement vector $r_i$. Note that if value of the certain skill in the vector is 0, it is not denoted for the sake of clarity.\n",
    "\n",
    "\n",
    "\n",
    "#### Starting courses\n",
    "\n",
    "1. Python Programming Basics\n",
    "    Requirements vector:\n",
    "        $r_{\\text{Python Programming Fundamentals}} = 0$\n",
    "    Upskill vector:\n",
    "        $u_{\\text{Python Programming Fundamentals}} = 2$\n",
    "\n",
    "2. Database Management Essentials\n",
    "    Requirements vector:\n",
    "        $r_{\\text{Python Programming Fundamentals}} = 2$\n",
    "    Upskill vector:\n",
    "        $u_{\\text{Database Management}} = 3$\n",
    "\n",
    "3. Introduction to Machine Learning with Python\n",
    "    Requirements vector:\n",
    "        $r_{\\text{Python Programming Fundamentals}} = 2$\n",
    "    Upskill vector:\n",
    "        $u_{\\text{Python Programming Fundamentals}} = 2$\n",
    "        $u_{\\text{Machine Learning}} = 4$\n",
    "        \n",
    "4. Web Development Using Python\n",
    "    Requirements vector:\n",
    "        $r_{\\text{Python Programming Fundamentals}} = 2$\n",
    "    Upskill vector:\n",
    "        $u_{\\text{Python Programming Fundamentals}} = 2$\n",
    "        $u_{\\text{Web Development}} = 3$\n",
    "       \n",
    "#### Advanced courses\n",
    "     \n",
    "1. Advanced Python Programming and Optimization\n",
    "    Requirements vector:\n",
    "        $r_{\\text{Python Programming Fundamentals}} = 3$\n",
    "    Upskill vector:\n",
    "        $u_{\\text{Python Programming Fundamentals}} = 4$\n",
    "\n",
    "2. Database Administration and Performance Tuning\n",
    "    Requirements vector:\n",
    "        $r_{\\text{Database Management}} = 3$\n",
    "        $r_{\\text{Python Programming Fundamentals}} = 3$\n",
    "    Upskill vector:\n",
    "        $u_{\\text{Database Management}} = 5$\n",
    "\n",
    "3. Deep Learning and Neural Networks with Python\n",
    "    Requirements vector:\n",
    "        $r_{\\text{Python Programming Fundamentals}} = 4$\n",
    "        $r_{\\text{Machine Learning}} = 3$\n",
    "    Upskill vector:\n",
    "        $u_{\\text{Python Programming Fundamentals}} = 3$\n",
    "        $u_{\\text{Machine Learning}} = 6$\n",
    "\n",
    "4. Full-Stack Web Development Mastery\n",
    "    Requirements vector:\n",
    "        $r_{\\text{Python Programming Fundamentals}} = 4$\n",
    "        $r_{\\text{Web Development}} = 4$\n",
    "    Upskill vector:\n",
    "        $u_{\\text{Python Programming Fundamentals}} = 3$\n",
    "        $u_{\\text{Web Development}} = 7$\n",
    "\n",
    "### Transition Probabilities\n",
    "\n",
    "Probability of passing a course is proportional to the dot product of the student's skill level and the course's requirement vector. \n",
    "- If the student passes the course, their skills are updated by $s + \\alpha \\cdot u_i \\cdot x$ where $x$ is a random soft mask and $\\alpha > \\beta$.\n",
    "- If the student fails, they might still get a slight skill improvement $s + \\beta \\cdot u_i \\cdot x$ where $\\beta < \\alpha$\n",
    "\n",
    "### Terminal State Probability\n",
    "\n",
    "The transition probability for passing a course can be represented as:  \n",
    "$$P(s, s^ \\prime, a_i) = \\frac{1}{1+ \\exp(-\\gamma r_i \\cdot s)} $$\n",
    "\n",
    "where $a_i$ is the action of taking course with requirement $r_i$ and course learning outcomes $r_i$  \n",
    "\n",
    "The state transition for passing the course:  \n",
    "$$ s^ \\prime = s + \\alpha \\cdot u_i \\odot x $$\n",
    "\n",
    "And for failing:  \n",
    "$$ s^ \\prime = s + \\beta \\cdot u_i \\odot x $$\n",
    "\n",
    "where $x$ is a mask that attenuates entries of $m$ by element wise multiplication $\\odot$ . Each student has its own $x$ such that each student has its own learning abilities.\n",
    "\n",
    "The terminal state probability involves the student reaching the desired minimal skill level in all domains, leading to a reward of +1.\n",
    "\n",
    "<!--!!!TODO\n",
    "### Rewards\n",
    "\n",
    "The rewards the agent recieves are based on the following factors:\n",
    "- Passing or failing a course\n",
    "- How long the agent is learning (cost of living)\n",
    "- Achieving the requested goal\n",
    "\n",
    "#### Passing the courses\n",
    "Passing the course yields a reward of 3 for the agent to make sure the agent focusses on courses it can pass. \n",
    "\n",
    "#### Failing the courses\n",
    "Failing the course yields a reward of 1 for the agent, since the agent can still learn a bit from failing a course.\n",
    "\n",
    "#### Cost of living\n",
    "To incentivize the agent to find the most efficient route a cost of living is taking into account. We made the decision to not implement this through states (as a budget for example) but through reward logic. Implementing this feature through states will lead to a lot more different states to calculate which is something we do not want, especially when the functionality is the same.\n",
    "\n",
    "!!!TODO-->\n",
    "\n",
    "### Rewards\n",
    "\n",
    "<!--!!!TODO !!!TODO-->\n",
    "\n",
    "## Implementation\n",
    "\n",
    "### Mdp Class\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "19bb174c9054a745"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "b085af6d9619a111"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Python `Mdp` Class is used mainly for storage of the States, Actions, Transition Probabilities and Rewards. In addition, it encapsulates logic of determining next State after Agents Action, for the Q-Learning and Monte-Carlo algorithms, helper methods for determining the available Actions for Agent and guard method for inspecting Transition Probabilities, so that they add up to 1 or 0 in case of the terminal state.\n",
    "\n",
    "The `Mdp` Class is also taking advantage of the `EnvironmentBuilder` Class to build States, Actions, Transition Probabilities and Rewards from the given input of MOOC structure and formulas written above.\n",
    "\n",
    "The MDP is devised to work with any type of the States and Actions. It is done by assigning to each given State or Action unique Id, which is used by the MDP for all operations. This allows to store the States and Actions in the NumPy Array, to save the computation resources and allow for great extensibility. In the implementation it is done via usage of the Python Lists, which store all the Possible States and Actions and therefore assign to each unique index.\n",
    "\n",
    "### EnvironmentBuilder Class"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dbf9a353baec809f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "7ee6b26f4a0ca5b0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<!--!!!TODO\n",
    "Description of EnvironmentBuilder\n",
    "!!!TODO-->\n",
    "\n",
    "### Solver Class"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "21650a2df145bac2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "723347496a25e3e9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "`Solver Class` is used to structure and make accessible algorithms for finding the Optimal Policy. It works based on the data stored in the `Mdp` Class and consists of implementation of Dynamic Programming, Q-Learning and Monte-Carlo. Q-Learning and Monte-Carlo are implemented with two operation modes: based on fixed amount of episodes or until Policy has Converged.\n",
    "\n",
    "The `Solver Class` is also responsible for generating the plots of the Action Value Function and Episode Returns. Each plot has visible Algorithm and Parameters used to obtain data, which allows for easy reruns of Algorithms and straightforward testing Algorithm performance.\n",
    "\n",
    "`Solver Class` is mainly used with combination of three steps:\n",
    "1. Resetting Solver to the initial values,\n",
    "2. Running selected Algorithm with given values,\n",
    "3. Displaying Plots.\n",
    "\n",
    "## Algorithms\n",
    "\n",
    "For the purpose of solving the problem we have implemented 3 different algorithms:\n",
    "\n",
    "- Dynamic Programming,\n",
    "- Q-Learning,\n",
    "- Monte-Carlo.\n",
    "\n",
    "Algorithms can be categorized based on their knowledge of transition probability. Dynamic Programming uses knowledge of all transition probabilities, where Q-Learning and Monte-Carlo work based on the data provided from mdp (states, actions, rewards).\n",
    "\n",
    "In addition, to standard implementation of Q-Learning and Monte-Carlo with the Epsilon exploration, the guards for starting stage of algorithms were introduced, as if all Actions in the current state have Action Value of 0, the Agent chooses random Action instead of always first one.\n",
    "`not np.max(self.action_value_array[previous_state, :])`\n",
    "\n",
    "### Dynamic Programming\n",
    "\n",
    "It is controlled by the variable `dynamic_programming_enabled` and does the Policy Evaluation and Policy Improvement to compute the Action Value Function based on the Greedy Policy with respect to Action Value Function, by implementation of the Bellman Equation:\n",
    "\n",
    "$$\n",
    "q_{\\pi}(s,a)=\\sum_{^{s^ \\prime \\in S}_{r \\in R}} p(s^ \\prime, r | s, a)[r + \\gamma \\sum_{a^ \\prime \\in A(s^ \\prime)} \\pi(a^ \\prime | s^ \\prime)q_{\\pi}(s^ \\prime | a^ \\prime)]\n",
    "$$\n",
    "\n",
    "The $\\sum_{a^ \\prime \\in A(s^ \\prime)} \\pi(a^ \\prime | s^ \\prime)q_{\\pi}(s^ \\prime | a^ \\prime)$ inside the code is solved by usage of `action_value_array[new_state, np.argmax(action_value_array[new_state, :])]`, as the Greedy Policy chooses only one action, not any other, which means that in probabilities given by $\\pi(a^ \\prime | s^ \\prime)$ they compose only of 0 and one 1, thus resulting in multiplying most of the values from the Action Value Function by 0. In order to omit unnecessary computation only the value of Action Value Function for the Action with probability of 1 is present.\n",
    "\n",
    "#### Policy Evaluation\n",
    "\n",
    "After each sweep through the Action Value Function the algorithm inspects whether maximal difference between old and new value is smaller than threshold `theta`, which indicates that the optimal Action Value Function for given policy has been found. Process stops repeating when the Policy has Converged.\n",
    "\n",
    "#### Policy Improvement / Policy Extraction\n",
    "\n",
    "Policy Improvement is done somewhat automatically, as the Policy is not stored, but is always calculated with the usage of `argmax()` function, which always returns Optimal Greedy Policy with respect to current Action Value Function.\n",
    "\n",
    "#### Algorithm's Performance\n",
    "\n",
    "Plots below showcase Algorithm Performance visualizing Action Value Function and Episode Returns. This presents <!--!!!TODO discribe algo performance !!!TODO-->\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3de21ef495c936bc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# dynamic programming\n",
    "solver.solve_with_dynamic_programming(theta=0.000001, gamma=0.9)\n",
    "solver.create_plot_of_action_value_array()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8cc79a15b2a42a18"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Q-Learning\n",
    "\n",
    "It is controlled by the variable `q_learning_enabled` and evaluates the Action Value Function every step of the agent. Algorithm inspects whether maximal difference between old and new value is smaller than threshold `theta`, which indicates that the optimal Action Value Function for given policy has been found. Process stops repeating when the Policy has Converged. Exploration rate of the Agent is controlled through `epsilon` parameter, which decides the probability of making random decision by Agent. Q-Learning Evaluation formula:\n",
    "\n",
    "<!--!!!TODO $$\n",
    "action_value_array[previous_state, action] = (1 - alpha_q_learning) * action_value_array[\n",
    "                previous_state, action] + alpha_q_learning * (reward + gamma * max(action_value_array[new_state, :]))\n",
    "$$ !!!TODO-->\n",
    "\n",
    "#### Algorithm's Performance\n",
    "\n",
    "Plots below showcase Algorithm Performance visualizing Action Value Function and Episode Returns. This presents <!--!!!TODO discribe algo performance !!!TODO-->\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7e99dadd34bc84b3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# q-learning\n",
    "solver.reset_solver()\n",
    "solver.solve_with_q_learning(True, True,\n",
    "                             episodes=10,\n",
    "                             theta=0.052,\n",
    "                             max_steps_in_episode=1000,\n",
    "                             start_state_index=0,\n",
    "                             alpha=0.04,\n",
    "                             epsilon=0.00,\n",
    "                             gamma=0.9,\n",
    "                             cost_of_living=0\n",
    "                             )\n",
    "solver.create_plot_of_action_value_array()\n",
    "solver.create_plot_of_episode_returns()\n",
    "\n",
    "solver.reset_solver()\n",
    "solver.solve_with_q_learning(True, True,\n",
    "                             episodes=10,\n",
    "                             theta=0.005,\n",
    "                             max_steps_in_episode=1000,\n",
    "                             start_state_index=0,\n",
    "                             alpha=0.04,\n",
    "                             epsilon=0.00,\n",
    "                             gamma=0.9,\n",
    "                             cost_of_living=0\n",
    "                             )\n",
    "solver.create_plot_of_action_value_array()\n",
    "solver.create_plot_of_episode_returns()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bc4b9eab6dd9b5ca"
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "### Monte-Carlo\n",
    "\n",
    "It is controlled by the variable `monte_carlo_enabled`. During the episode of the Agent, a history of choices is recorded, and after the episode finishes the Action Value Function is evaluated, by analysing the Agent path from the end to the beginning. Algorithm inspects whether maximal difference between old and new value is smaller than threshold `theta`, which indicates that the optimal Action Value Function for given policy has been found. Process stops repeating when the Policy has Converged. Exploration rate of the Agent is controlled through `epsilon` parameter, which decides the probability of making random decision by Agent. Monte-Carlo Evaluation formula:\n",
    "\n",
    "$$\n",
    "Q({s}^m_t, {a}^m_t) \\gets Q({s}^m_t, {a}^m_t) + \\alpha ({g}^m_t - Q({s}^m_t, {a}^m_t))\n",
    "$$\n",
    "\n",
    "#### Algorithm's Performance\n",
    "\n",
    "Plots below showcase Algorithm Performance visualizing Action Value Function and Episode Returns. This presents <!--!!!TODO discribe algo performance !!!TODO-->\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1b2415a32e6def0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# monte carlo\n",
    "solver.reset_solver()\n",
    "solver.solve_with_monte_carlo(True, True,\n",
    "                              episodes=10,\n",
    "                              theta=0.015,\n",
    "                              max_steps_in_episode=1000,\n",
    "                              start_state_index=0,\n",
    "                              alpha=0.04,\n",
    "                              epsilon=0.00,\n",
    "                              gamma=0.9,\n",
    "                              cost_of_living=0\n",
    "                              )\n",
    "solver.create_plot_of_action_value_array()\n",
    "solver.create_plot_of_episode_returns()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6fefdc4eee7ed828"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Conclusion\n",
    "\n",
    "The model is incorporating the test data, and the agent is learning the optimal path to achieve the goal, but the model need further testing on more diverse scenarios. The ground basics for the project has been laid out. When it comes to the Reinforcement Learning the project was really great experience that allowed for the \"hands on\" approach with the understanding of the underlying theoretical knowledge base behind it. All implemented concepts had their start within the theory, that was adapted to the real life scenario. Insight gained from this project will be a solid foundation for future development."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f4efb930e58e5906"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Additional Notes\n",
    "\n",
    "## Future Improvements\n",
    "\n",
    "The concept of this Report can be extended to compensate possible users with not just the optimal learning path for their goal, but also it could balance between user goal and the most optimal skills for the current market. This approach would mean sacrificing user expectations, for his own good and possible success.\n",
    "\n",
    "### Alternative Course structure\n",
    "\n",
    "For our project we have defined an MOOC course which is IT related. To show the possibilities this structure can offer we have also defined a math MOOC course constructed with different skills and courses to increase the understanding of said skills.\n",
    "\n",
    "Defined math skills:\n",
    "\n",
    "1. Arithmetic\n",
    "2. Geometry\n",
    "3. Algebra\n",
    "4. Calculus\n",
    "5. Statistics\n",
    "6. Discrete Mathematics\n",
    "7. Logic\n",
    "8. Mathematical Analysis\n",
    "\n",
    "Discriptive visualisation of the state: $(Arithmetic, Geometry, Algebra, Calculus, Statistics, Discrete Mathematics, Logic, Mathematical Analysis)$\n",
    "\n",
    "### Seperate courses/modules:\n",
    "\n",
    "#### Combo Modules\n",
    "1. **Statistical Methods in Number Theory for Beginners**\n",
    "\t-  Requirements vector:\n",
    "\t\t- $r_{Arithmetic} = 1$\n",
    "\t\t- $r_{Statistics} = 1$\n",
    "\t- Upskill vector:\n",
    "\t\t- $u_{Arithmetic} = 2$\n",
    "\t\t- $u_{Statistics} = 2$\n",
    "2. **Algebraic Statistics**\n",
    "\t-  Requirements vector:\n",
    "\t\t- $r_{Algebra} = 2$\n",
    "\t\t- $r_{Statistics} = 3$\n",
    "\t- Upskill vector:\n",
    "\t\t- $u_{Algebra} = 4$\n",
    "\t\t- $u_{Statistics} = 4$\n",
    "3. **Discrete Geometry and Logic**\n",
    "\t- Requirements vector:\n",
    "\t\t- $r_{Algebra} = 2$\n",
    "\t\t- $r_{Geometry} = 2$\n",
    "\t\t- $r_{Discrete Mathematics} = 2$\n",
    "\t\t- $r_{Logic} = 2$\n",
    "\t- Upskill vector:\n",
    "\t\t- $u_{DiscreteMathematics} = 3$\n",
    "\t\t- $u_{Logic} = 3$\n",
    "#### Arithmetic modules:\n",
    "1. **Basic Arithmetic**\n",
    "\t- Requirements vector:\n",
    "\t\t- None\n",
    "\t- Upskill vector:\n",
    "\t\t- $u_{Arithmetic} = 3$\n",
    "\t\t- $u_{Geometry} = 1$\n",
    "\t\t- $u_{Algebra} = 1$\n",
    "\t\t- $u_{Calculus} = 1$\n",
    "\n",
    "#### Algebra Modules\n",
    "1. **Elementary Algebra \n",
    "\t- Requirements vector:\n",
    "\t\t- None\n",
    "\t- Upskill vector:\n",
    "\t\t- $u_{Arithmetic} = 1$\n",
    "\t\t- $u_{Geometry} = 1$\n",
    "\t\t- $u_{Algebra} = 2$\n",
    "2. **Intermediate Algebra:**\n",
    "    - - Requirements vector:\n",
    "\t\t- $r_{Algebra} = 2$\n",
    "\t\t- $r_{Arithmetic} = 1$\n",
    "\t\t- $r_{Geometry} = 1$\n",
    "\t- Upskill vector:\n",
    "\t\t- $u_{Arithmetic} = 1$\n",
    "\t\t- $u_{Algebra} = 3$\n",
    "3. **Linear Algebra:**\n",
    "    - Requirements vector:\n",
    "\t\t- $r_{Algebra} = 5$\n",
    "\t\t- $r_{Arithmetic} = 3$\n",
    "\t\t- $r_{Geometry} = 3$\n",
    "\t\t- $r_{Calculus} = 2$\n",
    "\t- Upskill vector:\n",
    "\t\t- $u_{Arithmetic} = 1$\n",
    "\t\t- $u_{Geometry} = 1$\n",
    "\t\t- $u_{Algebra} = 3$\n",
    "\n",
    "#### Geometry modules \n",
    "1. **Geometry:**\n",
    "\t- Requirements vector:\n",
    "\t\t- $r_{Arithmetic} = 1$\n",
    "\t- Upskill vector:\n",
    "\t\t- $u_{Arithmetic} = 2$\n",
    "\t\t- $u_{Geometry} = 2$\n",
    "2. **Trigonometry:**\n",
    "    - Requirements vector:\n",
    "\t\t- $r_{Arithmetic} = 3$\n",
    "\t\t- $r_{Geometry} = 2$\n",
    "\t- Upskill vector:\n",
    "\t\t- $u_{Arithmetic} = 2$\n",
    "\t\t- $u_{Geometry} = 3$\n",
    "\n",
    "#### Calculus modules\n",
    "1. **Pre-Calculus:**\n",
    "    - Integrate algebra, geometry, and trigonometry to prepare for the study of calculus. Topics may include functions, limits, and basic mathematical modeling.\n",
    "    - Requirements vector:\n",
    "\t\t- $r_{Arithmetic} = 1$\n",
    "\t\t- $r_{Geometry} = 1$\n",
    "\t\t- $r_{Algebra} = 1$\n",
    "\t- Upskill vector:\n",
    "\t\t- $u_{Arithmetic} = 2$\n",
    "\t\t- $u_{Calculus} = 2$\n",
    "\t\t- $u_{Geometry} = 2$\n",
    "\t\t- $u_{Algebra} = 1$\n",
    "2. **Calculus:**\n",
    "    - Start with differential calculus, covering concepts like limits, derivatives, and applications. Then progress to integral calculus, exploring integrals and their applications.\n",
    "    - Requirements vector:\n",
    "\t\t- $r_{Arithmetic} = 2$\n",
    "\t\t- $r_{Geometry} = 2$\n",
    "\t\t- $r_{Algebra} = 2$\n",
    "\t\t- $r_{Calculus} = 2$\n",
    "\t- Upskill vector:\n",
    "\t\t- $u_{Arithmetic} = 1$\n",
    "\t\t- $u_{Calculus} = 3$\n",
    "\t\t- $u_{Geometry} = 1$\n",
    "\t\t- $u_{Algebra} = 1$\n",
    "3. **Differential Equations:**\n",
    "    - Study ordinary and partial differential equations and their applications in modeling real-world phenomena.\n",
    "    - Requirements vector:\n",
    "\t\t- $r_{Arithmetic} = 5$\n",
    "\t\t- $r_{Geometry} = 3$\n",
    "\t\t- $r_{Algebra} = 3$\n",
    "\t\t- $r_{Calculus} = 5$\n",
    "\t- Upskill vector:\n",
    "\t\t- $u_{Arithmetic} = 1$\n",
    "\t\t- $u_{Calculus} = 4$\n",
    "\t\t- $u_{Geometry} = 1$\n",
    "\t\t- $u_{Algebra} = 1$\n",
    "\n",
    "#### Statistics modules\n",
    "1. Probability and Statistics\n",
    "\t- Understand the principles of probability theory and statistical analysis. This is crucial for data analysis and decision-making in various fields.\n",
    "\t- Requirements vector:\n",
    "\t\t- $r_{Algebra} = 2$\n",
    "\t\t- $r_{Arithmetic} = 2$\n",
    "\t\t- $r_{Statistics} = 1$\n",
    "\t- Upskill vector:\n",
    "\t\t- $u_{Arithmetic} = 1$\n",
    "\t\t- $u_{Statistics} = 3$\n",
    "\n",
    "As you can see, a very extensive course structure can be build with this framework. The possibilities are endless.\n",
    "\n",
    "\n",
    "\n",
    "### Unforeseen Discoveries in Project Development\n",
    "\n",
    "#### Creating new formulas for the transition probabilites\n",
    "![passed](./passed.png \"passed\")\n",
    "![not passed](./not-passed.png \"not passed\")\n",
    "\n",
    "#### Redoing the same courses\n",
    "\n",
    "While working on our project, we stumbled upon something quite unexpected. Our learning agent ended up taking the same course multiple times to reach its skill goal. It was a bit of a head-scratcher, something we hadn't really thought about in the beginning. This experience taught us that creating a learning system isn't as straightforward as it might seem at first. \n",
    "\n",
    "This unexpected behavior made us realize the importance of thinking about all possible situations right from the start. No matter how well we plan, there will always be surprises. It's like trying to predict everything a student might do, not an easy task!\n",
    "\n",
    "This discovery also showed us that learning environments are kind of like puzzles. You have to make sure there are safeguards in place so things don't go off track. Testing our system in different situations became even more important.\n",
    "\n",
    "In the end, our project not only gave us a cool new learning framework but also showed us that learning is full of surprises. It made us think harder about how our system works and how we can make it even better."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f9456fc40417c047"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
